{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aakansha-Bansal/Capstone-Project---3-Classification/blob/main/Cardiovascular_risk_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Cardiovascular Risk Predection**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - **Classification**\n",
        "##### **Contribution**    - **Individual**\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The dataset is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The classification goal is to predict whether the patient has a 10-year risk of future coronary heart disease (CHD). The dataset provides the patients' information. It includes over 4,000 records and 15 attributes. Each attribute is a potential risk factor. There are both demographic, behavioral, and medical risk factors. we have got single CSV file as an input. we loaded a file using the pandas read csv function. The given data set contains 3390 rows and 17 columns. Then we checked missing/null values. By data wrangling, we find our data set contains a total of 3390 entries out of these 1923 are female and 1467 are males. Total age range between 32 to 70, out of 1923 females 1147 females do not smoke and 776 females do smoke. Out of 1467 males, 556 males do not smoke and 911 males do smoke. As per the data provided 2879 People have 10 years of risk of CHD disease, and 511 People do have not 10 years of risk of CHD disease. All these are columns data set overview. then we did a visualization of the processed data. We did hypothesize testing on some conclusions. For hypothesis, we choose the Z test i.e. When the variances are known and the sample size is high, a z-test is a statistical test to assess if two population means are different. A hypothesis test known as a z-test is one in which the z-statistic has a normal distribution. A z-statistic, also known as a z-score, is a numerical representation of the outcome of a z-test. Then we did feature engineering on our data set. we have handled missing values, replaced the null value with the mean or median, and after that, we looked for outliers. As we see the result our data contain so many outliers. we did treatment to remove outliers. For minimizing outliers we used the capping method, also known as the Winsorization method. Winsorization is a common technique for removing noise in experiment results, specifically from outliers. Winsorization refers to the practice of measuring the percentile Px of a metric and setting all values over Px to Px. At Statsig, the default percentile for winsorization is 99.9%. This reduces the influence of extreme outliers caused by factors such as logging errors or bad actors. then we did categorically encoding by astype() method is used to cast a pandas object to a specified dtype. astype() function also provides the capability to convert any suitable existing column to a categorical type. DataFrame. astype() function comes very handy when we want to case a particular column data type to another data type, Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. It is performed during the data pre-processing to handle highly varying magnitudes or values or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, and higher and consider smaller values as the lower values, regardless of the unit of the values. Min-Max Normalization: This technique re-scales a feature or observation value with a distribution value between 0 and 1. at the end we implement our model. GridSearchCV is the process of performing hyperparameter tuning in order to determine the optimal values for a given model. As mentioned above, the performance of a model significantly depends on the value of hyperparameters. Note that there is no way to know in advance the best values for hyperparameters so ideally, we need to try all possible values to know the optimal values. Doing this manually could take a considerable amount of time and resources and thus we use GridSearchCV to automate the tuning of hyperparameters."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(https://github.com/Aakansha-Bansal/Capstone-Project---3-Classification)"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The dataset is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The classification goal is to predict whether the patient has a 10-year risk of future coronary heart disease (CHD). The dataset provides the patients' information. It includes over 4,000 records and 15 attributes. Each attribute is a potential risk factor. There are both demographic, behavioral, and medical risk factors.**\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Installing Libraries\n",
        "# !pip install lime\n",
        "# !pip install plotly==5.13.0\n",
        "#import all the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import ttest_1samp  \n",
        "# from lime import lime_tabular\n",
        "from numpy import math\n",
        "from scipy.stats import *\n",
        "import math\n",
        "from scipy.stats import ttest_1samp  \n",
        "from sklearn import  svm,datasets\n",
        "from sklearn.model_selection import  GridSearchCV\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from sklearn import preprocessing\n",
        "\n",
        "#Visualiztion libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Import The Date Class\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "from sklearn.linear_model import LogisticRegression  \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "#for model building \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import XGBRFClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn import metrics\n",
        "# from lime import lime_tabular\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "#for model evaluation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import log_loss\n",
        "from xgboost import XGBClassifier\n",
        "import pandas as pd\n",
        "\n",
        "import sklearn\n",
        "\n",
        "!pip install shap==0.40.0\n",
        "# import shap \n",
        "import graphviz\n",
        "sns.set_style('darkgrid') \n",
        "!pip3 install xgboost\n",
        "!pip install interpret\n",
        "\n",
        "import interpret.glassbox\n",
        "import shap\n",
        "#---- For handling warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "9Pe3Nma4PFLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2bAnzIW2AN2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "heart_df=pd.read_csv('/content/drive/MyDrive/data_cardiovascular_risk.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "heart_df"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heart_df.head()"
      ],
      "metadata": {
        "id": "gaGr7g2OCw0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "heart_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "heart_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(heart_df[heart_df.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "heart_df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(heart_df.isnull(), cbar=True)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As we have seen in our dataset there are many null values present in many columns and most of the null values present are in glucose column i.e. 304 null values.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "heart_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "heart_df.describe(include=\"all\").T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Sex - gender\n",
        "\n",
        "Age - age\n",
        "\n",
        "is_smoking - Whether smoking currently or not\n",
        "\n",
        "Cigs Per Day - Cigarettes smoked per day\n",
        "\n",
        "BP_Meds - Whether taking BP meds or not\n",
        "\n",
        "Prevalent Stroke - If the patient has a history of stroke\n",
        "\n",
        "Prevalent hyp - If the patient has a history of hypertension\n",
        "\n",
        "Diabetes - Patient has diabetes or not\n",
        "\n",
        "Tot Chol - Cholesterol measure\n",
        "\n",
        "Sys BP - BP measure\n",
        "\n",
        "Dia BP - BP measure\n",
        "\n",
        "BMI - Body Mass Index\n",
        "\n",
        "Heart Rate - Heart Rate measure"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "heart_df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Male & Female count \n",
        "male_female_count=heart_df.sex.value_counts().reset_index()\n",
        "male_female_count\n",
        "     "
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total patient count  has a 10-year risk of future coronary heart disease (CHD) or not\n",
        "chd_count=heart_df.TenYearCHD.value_counts().reset_index()\n",
        "chd_count"
      ],
      "metadata": {
        "id": "BI0UvT2GGLCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Total age count in given data set\n",
        "age_count=heart_df.age.value_counts().reset_index()\n",
        "\n",
        "print(f\"Maximum age is = {age_count['index'].max()}\")\n",
        "print(f\"Minimum age is = {age_count['index'].min()}\")\n",
        "age_count"
      ],
      "metadata": {
        "id": "xfZODPeYGQ6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total count of people wheather currently smoking or not as per gender\n",
        "smoking_vs_gender=heart_df.groupby(['sex'])['is_smoking'].value_counts().unstack()\n",
        "smoking_vs_gender"
      ],
      "metadata": {
        "id": "v9uYnhx7Jcwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avg total count of smoking as on age\n",
        "cig_vs_age =heart_df.groupby(['age'])['cigsPerDay'].mean()\n",
        "cig_vs_age=round(cig_vs_age,2).reset_index()\n",
        "cig_vs_age\n",
        "     "
      ],
      "metadata": {
        "id": "7_yzQQ_0KIQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aveg Cholestol  per age\n",
        "chol_vs_age = heart_df.groupby(['age'])['totChol'].mean()\n",
        "chol_vs_age=round(chol_vs_age,2).reset_index()\n",
        "chol_vs_age"
      ],
      "metadata": {
        "id": "QqicTZftKQSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avg blood pressure as per age\n",
        "bp_vs_age = heart_df.groupby(['age'])['sysBP'].mean()\n",
        "bp_vs_age=round(bp_vs_age,2).reset_index()\n",
        "bp_vs_age"
      ],
      "metadata": {
        "id": "WK3kWOrHF0K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avg heartrate as per age\n",
        "hrtrate_vs_age= round(heart_df.groupby(['age'])['heartRate'].mean(),2).reset_index()\n",
        "hrtrate_vs_age"
      ],
      "metadata": {
        "id": "FYAgXqbWGD85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avg risk factor as per age\n",
        "age_vs_riskfactor=heart_df.groupby(['age'])['totChol', 'sysBP',\n",
        "       'diaBP', 'BMI', 'heartRate', 'glucose'].mean()\n",
        "age_vs_riskfactor"
      ],
      "metadata": {
        "id": "o7qteqQhGhLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As of now, we have done the required manipulations for data cleaning."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Male and Female counts.**"
      ],
      "metadata": {
        "id": "P_SLrPEsHtlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "fig,ax=plt.subplots(figsize=(7,7))\n",
        "sns.barplot(data=male_female_count,x='index',y='sex',ax=ax,capsize=.2)\n",
        "ax.set(title='Male-Female Count ')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A bar chart is a chart that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent. The bars can be plotted vertically or horizontally. A vertical bar chart is sometimes called a column chart."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### It gives us that there are 1923 females and 1467 males are present in our dataset i.e. total of 3390."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As we can see from above graph we can say that female have more chances of CHD than male."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Future risk of CHD**"
      ],
      "metadata": {
        "id": "qFlBiXvawFV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "#countplot of dependent variable\n",
        "sns.countplot(x=heart_df['TenYearCHD'])\n",
        "plt.title('Ten year cardiovascular heart disease distribution');"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A bar chart is a chart that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent. The bars can be plotted vertically or horizontally. A vertical bar chart is sometimes called a column chart."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2879 People have 10 year CHD disease.\n",
        "### 511 People have not 10 year CHD disease."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2879 Out of 3390 people have 10 yr CHD disease chances which almost 85+%."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Total age count**"
      ],
      "metadata": {
        "id": "fniLSdqgwQIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "fig = px.pie(age_count, values='index', names='index', title='Total unique age count')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A pie chart expresses a part-to-whole relationship in your data. It's easy to explain the percentage comparison through area covered in a circle with different colors. Where differenet percentage comparison comes into action pie chart is used frequently. So, I used Pie chart and which helped me to get the percentage comparision of the dependant variable.\n",
        "\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The minimum age in given data set is 32 and maximum age 72.\n",
        "### Total 148 people entries available from age group 40."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Our data set contain mostly entries from age range 40 to 45.\n",
        "\n"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Heatmap Chart ( is smoking vs gender) - 4\n"
      ],
      "metadata": {
        "id": "5-dwYVENJJe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "smoking_vs_gender_chart=sns.heatmap(smoking_vs_gender, cmap='RdBu' , annot=True , fmt=\".0f\",linewidths=1 )\n",
        "print(smoking_vs_gender_chart)"
      ],
      "metadata": {
        "id": "XMYxWwYcJEkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Why did you pick the specific chart?\n",
        "### A heat map is a two-dimensional representation of data in which values are represented by colors. A simple heat map provides an immediate visual summary of information. More elaborate heat maps allow the viewer to understand complex data sets."
      ],
      "metadata": {
        "id": "dfnxh852KmPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. What is/are the insight(s) found from the chart?\n",
        "## 1147 out of 1923 female do smoking.\n",
        "### 776 out of 1923 female do not smoking.\n",
        "### 556 out of 1467 male do smoking.\n",
        "### 911 out of 1467 male do not smoking."
      ],
      "metadata": {
        "id": "uFLzVGC8KqBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Will the gained insights help creating a positive business impact?\n",
        "### Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "### As we see above chart we can understand female smokes more than males."
      ],
      "metadata": {
        "id": "d8dEza0PLALn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **cigratte consume as per age**"
      ],
      "metadata": {
        "id": "DDlfg3iZwocL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "fig = px.funnel(cig_vs_age, x='age', y='cigsPerDay')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A funnel plot is a graph designed to check for the existence of publication bias; funnel plots are commonly used in systematic reviews and meta-analyses. In the absence of publication bias, it assumes that studies with high precision will be plotted near the average, and studies with low precision will be spread evenly on both sides of the average, creating a roughly funnel-shaped distribution. Deviation from this shape can indicate publication bias."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 32 age group people consume 15 cigratte per day.\n",
        "### 70 age group people consume 0 cigratte per day."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As we can see in above graph as age increases number of cigratte consume per day decreases."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cholestrol as per age**"
      ],
      "metadata": {
        "id": "0enD05k9yI94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "fig = px.bar(chol_vs_age, x='age', y='totChol')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This chart gives proper visulization about the Cholestrol as per age."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 62 age group people have average 260 cholesteral measure and which is high among all age group.\n",
        "### 70 age group people have average 169 cholesteral measure and which is low among all age group."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Age range from 50 to 65 the cholesteral level is nearly equal to 250."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Average BP measure as per age "
      ],
      "metadata": {
        "id": "zBT5fAa9yzPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "fig = px.pie(bp_vs_age, values='sysBP', names='age', color_discrete_sequence=px.colors.sequential.RdBu)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A pie chart expresses a part-to-whole relationship in your data. It's easy to explain the percentage comparison through area covered in a circle with different colors. Where differenet percentage comparison comes into action pie chart is used frequently. So, I used Pie chart and which helped me to get the percentage comparision of the dependant variable."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Average BP measure higher which 147 at age group 61.\n",
        "### Average BP measure lower which 111 at age group 31."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From above pie chart we can say that as the age increases BP level also increases."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Average heartrate as per age"
      ],
      "metadata": {
        "id": "i2Uc79nCzwaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "fig = px.bar(hrtrate_vs_age, x='age', y='heartRate',\n",
        "             hover_data=['heartRate', 'age'], color='heartRate',\n",
        "             labels={'pop':'population of Canada'}, height=400)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A bar chart or bar graph is a chart or graph that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent. The bars can be plotted vertically or horizontally. A vertical bar chart is sometimes called a column chart."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Heart rate is higher for age group 32 and 69 which is nearly equal to 80.\n",
        "### Lowest heart rate recorded for age group 70 which is 64."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exclusing age group 32, 69 and 70 all age group have normal heartrate measure which is nearly equal to 72."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Risk factor**"
      ],
      "metadata": {
        "id": "LxSsnDWb0ZnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "fig = go.Figure(data=[go.Surface(z=age_vs_riskfactor.values)])\n",
        "fig.update_traces(contours_z=dict(show=True, usecolormap=True,\n",
        "                                  highlightcolor=\"limegreen\", project_z=True))\n",
        "fig.update_layout(title='Risk factor as on age', autosize=False,\n",
        "                  scene_camera_eye=dict(x=1.87, y=0.88, z=-0.64),\n",
        "                  width=500, height=500,\n",
        "                  margin=dict(l=65, r=50, b=65, t=90)\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3D chart represents quantitative information. The chart consists of horizontally aligned rectangular bars of equal width with lengths proportional to the values they represent, something that aids in instant comparison of data. One axis of the chart plots categories and the other axis represents the value scale."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The insight is that the risk of ten year CHD increases with increase in age."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(18,10))\n",
        "sns.heatmap(heart_df.corr(),annot=True)\n",
        "plt.title('Co-relation of the columns')"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To visualise the relationship between variables . A heatmap is a effective tool to identify highly correlated variables.A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses. The range of correlation is [-1,1]."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In the above chart we see correlation between the all columns :\n",
        "\n",
        "###The variables systolic BP and diastolic BP are highly correlated.\n",
        "### The variables diabetes and glucose are highly correlated.\n",
        "### The variables systolic BP, diastolic BP and prevalent_hyp are highly correlated.\n",
        "\n"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 - Pair Plot "
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(heart_df)"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pairplot visualizes given data to find the relationship between them where the variables can be continuous or categorical. Plot pairwise relationships in a data-set.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Above chart shows correlation between columns."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Null Hypothesis: N = 132\n",
        "\n",
        "### Alternate Hypothesis : N < 132\n",
        "\n",
        "### Test Type: Z test"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn import metrics\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from scipy.stats import *\n",
        "import math\n",
        "from statsmodels.stats import weightstats as stests\n",
        "print(heart_df['sysBP'].mean())\n",
        "heart_df[['sysBP']].describe()\n",
        "ztest ,propability_value = stests.ztest(heart_df['sysBP'], x2=None, value=132)\n",
        "print(f\"P Value is : {float(propability_value)}\")\n",
        "if propability_value<0.05:\n",
        "    print(\"Null hyphothesis rejected , Alternative hyphothesis accepted\")\n",
        "else:\n",
        "    print(\"Null hyphothesis accepted , Alternative hyphothesis rejected\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## When the variances are known and the sample size is high, a z-test is a statistical test to assess if two population means are different. A hypothesis test known as a z-test is one in which the z-statistic has a normal distribution. A z-statistic, also known as a z-score, is a numerical representation of the outcome of a z-test."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig=plt.figure(figsize=(9,6))\n",
        "ax=fig.gca()\n",
        "feature= (heart_df['sysBP'])\n",
        "sns.distplot(heart_df['sysBP'])\n",
        "ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "ax.set_title('sysBP')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hg4faIuS6q-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As shown in the figure the mean is approximately same as the median. Thus, it is a Normal Distribution. That's why I have used Z-Test directly."
      ],
      "metadata": {
        "id": "dKcGRaK26sYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Null Hypothesis: N = 82\n",
        "\n",
        "### Alternate Hypothesis : N < 82\n",
        "\n",
        "### Test Type: Z test"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "print(heart_df['diaBP'].mean())\n",
        "heart_df[['diaBP']].describe()\n",
        "ztest ,propability_value = stests.ztest(heart_df['diaBP'], x2=None, value=82)\n",
        "print(f\"P Value is : {float(propability_value)}\")\n",
        "if propability_value<0.05:\n",
        "    print(\"Null hyphothesis rejected , Alternative hyphothesis accepted\")\n",
        "else:\n",
        "    print(\"Null hyphothesis accepted , Alternative hyphothesis rejected\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When the variances are known and the sample size is high, a z-test is a statistical test to assess if two population means are different. A hypothesis test known as a z-test is one in which the z-statistic has a normal distribution. A z-statistic, also known as a z-score, is a numerical representation of the outcome of a z-test.\n",
        "\n"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig=plt.figure(figsize=(9,6))\n",
        "ax=fig.gca()\n",
        "feature= (heart_df['diaBP'])\n",
        "sns.distplot(heart_df['diaBP'])\n",
        "ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "ax.set_title('diaBP')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qmXMCzJb7evz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The mean and median are about the same, as indicated in the image. It is a Normal Distribution as a result. I immediately utilised Z-Test for this reason."
      ],
      "metadata": {
        "id": "x5k5635Z7m4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Null Hypothesis: N = 72\n",
        "\n",
        "### Alternate Hypothesis : N < 72\n",
        "\n",
        "### Test Type: Z test"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "print(heart_df['glucose'].mean())\n",
        "heart_df[['glucose']].describe()\n",
        "ztest ,propability_value = stests.ztest(heart_df['glucose'], x2=None, value=72)\n",
        "print(f\"P Value is : {float(propability_value)}\")\n",
        "if propability_value<0.05:\n",
        "    print(\"Null hyphothesis rejected , Alternative hyphothesis accepted\")\n",
        "else:\n",
        "    print(\"Null hyphothesis accepted , Alternative hyphothesis rejected\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A z-test is a statistical test to determine if two population means are different when the variances are known and the sample size is large. When the z-statistic has a normal distribution, a hypothesis test is referred to as a z-test. The result of a z-test is represented numerically by a z-statistic, sometimes referred to as a z-score."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig=plt.figure(figsize=(9,6))\n",
        "ax=fig.gca()\n",
        "feature= (heart_df['glucose'])\n",
        "sns.distplot(heart_df['glucose'])\n",
        "ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "ax.set_title('heartRate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mn7thTzg8Jdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The mean and median are about the same, as indicated in the image. It is a Normal Distribution as a result. I immediately utilised Z-Test for this reason.\n",
        "\n"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Creating a copy of the dataset for further feature engineering\n",
        "df=heart_df.copy()"
      ],
      "metadata": {
        "id": "21TmdQOT8edW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Visualizing the missing values\n",
        "# Checking Null Value by plotting Heatmap\n",
        "sns.heatmap(df.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling null values in columns BPMeds\n",
        "round(df.groupby(['sex'])['BPMeds'].mean(),1)\n",
        "df['BPMeds'] = df['BPMeds'].fillna(0)"
      ],
      "metadata": {
        "id": "juSTgxp786AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling null values in columns cigsPerDay\n",
        "round(df.groupby(['sex'])['cigsPerDay'].mean(),1)\n",
        "\n",
        "# Defining Function For  cigsPerDay columns\n",
        "def fill_cigperday(cols):\n",
        "  cigsPerDay=cols[0]\n",
        "  sex=cols[1]\n",
        "  if pd.isnull(cigsPerDay):\n",
        "    if sex==0:\n",
        "      return 5\n",
        "    else:\n",
        "      return 13\n",
        "  else:\n",
        "    return  cigsPerDay     \n",
        "\n",
        "# Filling null values in columns cigsPerDay\n",
        "\n",
        "df['cigsPerDay']=df[['cigsPerDay','sex']].apply(fill_cigperday,axis=1)"
      ],
      "metadata": {
        "id": "azgdoDR_9GFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling null values in columns sex\n",
        "round(df.groupby(['sex'])['totChol'].mean(),1)\n",
        "\n",
        "# Defining Function For  sex columns\n",
        "\n",
        "def fill_totchol(cols):\n",
        "  totChol=cols[0]\n",
        "  sex=cols[1]\n",
        "  if pd.isnull(totChol):\n",
        "    if sex==0:\n",
        "      return 239\n",
        "    else:\n",
        "      return 233\n",
        "  else:\n",
        "    return  totChol   \n",
        "# Filling null values in columns sex\n",
        "df['totChol']=df[['totChol','sex']].apply(fill_totchol,axis=1)    "
      ],
      "metadata": {
        "id": "6GgAQp4q9dWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling null values in columns BMI\n",
        "round(df.groupby(['sex'])['BMI'].mean(),1)\n",
        "\n",
        "# Defining Function For  BMI columns\n",
        "\n",
        "def fill_BMI(cols):\n",
        "  BMI=cols[0]\n",
        "  sex=cols[1]\n",
        "  if pd.isnull(BMI):\n",
        "    if sex==0:\n",
        "      return 25\n",
        "    else:\n",
        "      return 26\n",
        "  else:\n",
        "    return  BMI   \n",
        "# Filling null values in columns BMI\n",
        "df['BMI']=df[['BMI','sex']].apply(fill_BMI,axis=1)    "
      ],
      "metadata": {
        "id": "8N0LLro59nSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling null values in columns heartRate\n",
        "\n",
        "round(df.groupby(['sex'])['heartRate'].mean(),1)\n",
        "\n",
        "# Defining Function For  heartrate columns\n",
        "\n",
        "def fill_heartrate(cols):\n",
        "  heartRate=cols[0]\n",
        "  sex=cols[1]\n",
        "  if pd.isnull(heartRate):\n",
        "    if sex==0:\n",
        "      return 77\n",
        "    else:\n",
        "      return 74\n",
        "  else:\n",
        "    return  heartRate \n",
        "\n",
        "# Filling null values in columns heartRate\n",
        "\n",
        "df['heartRate']=df[['heartRate','sex']].apply(fill_heartrate,axis=1)  "
      ],
      "metadata": {
        "id": "Pvv7hs8y-CpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling null values in columns glucose\n",
        "\n",
        "round(df.groupby(['sex'])['glucose'].mean(),1)\n",
        "\n",
        "\n",
        "# Defining Function For  glucose columns\n",
        "\n",
        "\n",
        "def fill_glucose(cols):\n",
        "  glucose=cols[0]\n",
        "  sex=cols[1]\n",
        "  if pd.isnull(glucose):\n",
        "    if sex==0:\n",
        "      return 77\n",
        "    else:\n",
        "      return 74\n",
        "  else:\n",
        "    return  glucose \n",
        "\n",
        "# Filling null values in columns glucose\n",
        "\n",
        "df['glucose']=df[['glucose','sex']].apply(fill_glucose,axis=1)"
      ],
      "metadata": {
        "id": "F_7s5N0u-J2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Null value count after missing value imputation\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Visualizing the missing values\n",
        "# Checking Null Value by plotting Heatmap\n",
        "sns.heatmap(df.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "Xp_JCoCY-Q99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "numerical_columns=list(df.describe())\n",
        "numerical_features=pd.Index(numerical_columns)\n",
        "numerical_features\n",
        "\n",
        "for col in numerical_features:\n",
        "  plt.figure(figsize=(9,6))\n",
        "  sns.boxplot(x=df[col])\n",
        "  \n",
        "  plt.title(col)\n",
        "  plt.tight_layout()\n",
        "  \n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# heart rate column Before capping\n",
        "df['heartRate'].describe()"
      ],
      "metadata": {
        "id": "vq61VHTSfuMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upper_limit=df['heartRate'].quantile(0.99)\n",
        "print(upper_limit)\n",
        "lower_limit=df['heartRate'].quantile(0.01)\n",
        "print(lower_limit)"
      ],
      "metadata": {
        "id": "vTQwnLbEf174"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['heartRate']=np.where(df['heartRate']>=upper_limit,upper_limit,\n",
        "         np.where(df['heartRate'] <= lower_limit,lower_limit,\n",
        "                  df['heartRate']))\n",
        "# After Capping \n",
        "\n",
        "df['heartRate'].describe()"
      ],
      "metadata": {
        "id": "U3ZonY19f8eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column BMI before capping\n",
        "df['BMI'].describe()"
      ],
      "metadata": {
        "id": "4h_3qfvhgH73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upper_limit=df['BMI'].quantile(0.99)\n",
        "print(upper_limit)\n",
        "lower_limit=df['BMI'].quantile(0.01)\n",
        "print(lower_limit)"
      ],
      "metadata": {
        "id": "xy4BLZwYgLIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['BMI']=np.where(df['BMI']>=upper_limit,upper_limit,\n",
        "         np.where(df['BMI'] <= lower_limit,lower_limit,\n",
        "                  df['BMI']))\n",
        "\n",
        "#  BMI column after capping\n",
        "df['BMI'].describe()"
      ],
      "metadata": {
        "id": "wBr7R3xPgcNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# column glucose before capping\n",
        "df['glucose'].describe()"
      ],
      "metadata": {
        "id": "wDZBsVggg1yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upper_limit=df['glucose'].quantile(0.99)\n",
        "print(upper_limit)\n",
        "lower_limit=df['glucose'].quantile(0.01)\n",
        "print(lower_limit)"
      ],
      "metadata": {
        "id": "7D1xRaw2g9NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['glucose']=np.where(df['glucose']>=upper_limit,upper_limit,\n",
        "         np.where(df['glucose'] <= lower_limit,lower_limit,\n",
        "                  df['glucose']))\n",
        "#  Glucose column after capping\n",
        "df['glucose'].describe()"
      ],
      "metadata": {
        "id": "t_4dwxo0hB3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# columns diabp before capping\n",
        "df['diaBP'].describe()  "
      ],
      "metadata": {
        "id": "Y-XvO6U_hJh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upper_limit=df['diaBP'].quantile(0.99)\n",
        "print(upper_limit)\n",
        "lower_limit=df['diaBP'].quantile(0.01)\n",
        "print(lower_limit)"
      ],
      "metadata": {
        "id": "xOp168lhhOhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['diaBP']=np.where(df['diaBP']>=upper_limit,upper_limit,\n",
        "         np.where(df['diaBP'] <= lower_limit,lower_limit,\n",
        "                  df['diaBP']))\n",
        "\n",
        "# diaBP columns after capping \n",
        "df['diaBP'].describe()"
      ],
      "metadata": {
        "id": "kHrnP-yRhT2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# column totchol before capping\n",
        "df['totChol'].describe()"
      ],
      "metadata": {
        "id": "jpXBJTe7heL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upper_limit=df['totChol'].quantile(0.99)\n",
        "print(upper_limit)\n",
        "lower_limit=df['totChol'].quantile(0.01)\n",
        "print(lower_limit)"
      ],
      "metadata": {
        "id": "Jmo1lEZQhiKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['totChol']=np.where(df['totChol']>=upper_limit,upper_limit,\n",
        "         np.where(df['totChol'] <= lower_limit,lower_limit,\n",
        "                  df['totChol']))\n",
        "# totChol columns after capping\n",
        "df['totChol'].describe()"
      ],
      "metadata": {
        "id": "JzH4mJI-hol-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  column sybp before capping\n",
        "df['sysBP'].describe()"
      ],
      "metadata": {
        "id": "IqEb3Am3hu3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upper_limit=df['sysBP'].quantile(0.99)\n",
        "print(upper_limit)\n",
        "lower_limit=df['sysBP'].quantile(0.01)\n",
        "print(lower_limit)"
      ],
      "metadata": {
        "id": "T0hoCOz5hy2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sysBP']=np.where(df['sysBP']>=upper_limit,upper_limit,\n",
        "         np.where(df['sysBP'] <= lower_limit,lower_limit,\n",
        "                  df['sysBP']))\n",
        "\n",
        "# sysBP column after capping\n",
        "df['sysBP'].describe()"
      ],
      "metadata": {
        "id": "4ZHtPEJPh4rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Winsorization is a common technique for removing noise in experiment results, specifically from outliers.\n",
        "\n",
        "### Winsorization refers to the practice of measuring the percentile Px of a metric and setting all values over Px to Px. At Statsig, the default percentile for winsorization is 99.9%. This reduces the influence of extreme outliers caused by factors such as logging errors or bad actors."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Getting the categorical columns\n",
        "categorical_columns=list(set(df.columns.to_list()).difference(set(df.describe().columns.to_list())))\n",
        "print(\"Categorical Columns are :-\", categorical_columns)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Gender column label encding\n",
        "df[ 'sex' ] = df[ 'sex' ].astype( 'category' )  \n",
        "print(\"\\nData Frame after Label Encoding using Category codes:\\n\")  \n",
        "df[ 'sex' ] = df[ 'sex' ].cat.codes  \n",
        "df[ 'sex' ] "
      ],
      "metadata": {
        "id": "FSyNrjWgiUik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  is_smoking column label encding\n",
        "df[ 'is_smoking' ] = df[ 'is_smoking' ].astype( 'category' )  \n",
        "print(\"\\nData Frame after Label Encoding using Category codes:\\n\")  \n",
        "df[ 'is_smoking' ] = df[ 'is_smoking' ].cat.codes  \n",
        "df[ 'is_smoking' ] "
      ],
      "metadata": {
        "id": "K6cz_a55if_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data set View after label encoding\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ndSYJhPbiqg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### astype() method is used to cast a pandas object to a specified dtype. astype() function also provides the capability to convert any suitable existing column to categorical type. DataFrame. astype() function comes very handy when we want to case a particular column data type to another data type."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "df.columns"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "N0PLeScOjI7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing  column  'education' from  data set\n",
        "df.drop(['education'], axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "JTeEse0QjPW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset after removing column 'education'\n",
        "df.head()"
      ],
      "metadata": {
        "id": "pVs1b_w8jSYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "origional_feature=df.columns\n",
        "print(\"Origional feature count\", len(origional_feature))\n",
        "print(\"origional feature :\", origional_feature)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting co realtion heat map\n",
        "plt.figure(figsize=(18, 8))\n",
        "dataplot = sns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True)\n",
        "  \n",
        "# displaying heatmap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ful0og4Qji4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### By Using Correlation chart we have selected features."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# checking vif value \n",
        "\n",
        "X = df.iloc[: ,1 :-1]\n",
        "  \n",
        "# VIF dataframe\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "  \n",
        "# calculating VIF for each feature\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n",
        "                          for i in range(len(X.columns))]\n",
        "  \n",
        "print(vif_data)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature scaling\n",
        "\n",
        "fig=plt.figure(figsize=(9,6))\n",
        "ax=fig.gca()\n",
        "feature= (X.all())\n",
        "sns.distplot(X.all())\n",
        "ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "ax.set_title(\"Feature scaling\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BJcwEiXRj8kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature=['age','sex','is_smoking','cigsPerDay','totChol','sysBP','diaBP','BMI','heartRate','glucose']\n",
        "# independent variable\n",
        "X = df[feature]\n",
        "\n",
        "print(\"Independent Variable: \")\n",
        "# X = df.iloc[: ,1 :-1]\n",
        "X"
      ],
      "metadata": {
        "id": "oYcMzO0fkCFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dependent variable\n",
        "feature=['TenYearCHD']\n",
        "y=df[feature]\n",
        "print(\"dependent Variable: \")\n",
        "y"
      ],
      "metadata": {
        "id": "GnVqtwIDkIOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. It is performed during the data pre-processing to handle highly varying magnitudes or values or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values. Min-Max Normalization: This technique re-scales a feature or observation value with distribution value between 0 and 1."
      ],
      "metadata": {
        "id": "TLYwVh0RkTb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# split into 70:30 ratio\n",
        "x_train, x_test, y_train, y_test= train_test_split(X, y, test_size= 0.30, random_state=0)  "
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# describes info about train and test set\n",
        "print(\"Number transactions X_train dataset: \", x_train.shape)\n",
        "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
        "print(\"Number transactions X_test dataset: \", x_test.shape)\n",
        "print(\"Number transactions y_test dataset: \", y_test.shape)"
      ],
      "metadata": {
        "id": "d_wyTgg7k8ZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### There are two competing concerns: with less training data, your parameter estimates have greater variance. With less testing data, your performance statistic will have greater variance. Broadly speaking you should be concerned with dividing data such that neither variance is too high, which is more to do with the absolute number of instances in each category rather than the percentage.\n",
        "\n",
        "### In this case the training dataset is large, that's why I have taken 70:30 ratio."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Logistic Classification**"
      ],
      "metadata": {
        "id": "j8lyLMyrlMVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "classifier= LogisticRegression(random_state=0)\n",
        "# Fit the Algorithm\n",
        "classifier.fit(x_train, y_train)  "
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the coefficients\n",
        "classifier.coef_"
      ],
      "metadata": {
        "id": "ot7Cj8ZVldv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the intercept value\n",
        "classifier.intercept_"
      ],
      "metadata": {
        "id": "HuffMuYSlgJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "\n",
        "train_preds = classifier.predict_proba(x_train)\n",
        "test_preds = classifier.predict_proba(x_test)"
      ],
      "metadata": {
        "id": "GZ1knVvflh5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_preds = classifier.predict(x_train)\n",
        "test_preds = classifier.predict(x_test)"
      ],
      "metadata": {
        "id": "-kby6Lpsl0JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the accuracy scores\n",
        "train_accuracy = accuracy_score(train_preds,y_train)\n",
        "test_accuracy = accuracy_score(test_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on train data is \", train_accuracy)\n",
        "print(\"The accuracy on test data is \", test_accuracy)"
      ],
      "metadata": {
        "id": "pC6GOKERl2ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual vs Predicted visualization\n",
        "plt.figure(figsize=(19,10))\n",
        "plt.plot((test_preds), color=\"darkblue\")\n",
        "plt.plot(np.array((y_test)),color=\"lightgreen\")\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4zJiC-XKl9Nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix:\n",
        "It is a table with combinations of predicted and actual values. predictions and the total number of predictions."
      ],
      "metadata": {
        "id": "jaXZkkegl3Ib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "confusion_matrix(y_train, train_preds)"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy:\n",
        "Accuracy simply measures how often the classifier correctly predicts. We can define accuracy as the ratio of the number of correct predictions and the total number of predictions.\n",
        "Accuracy = (True Positive+True Negative)/(True Positive+True Negative+False Positive+False Negative)\n"
      ],
      "metadata": {
        "id": "79lgFo_smEAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy = \",metrics.accuracy_score(y_train, train_preds)*100)"
      ],
      "metadata": {
        "id": "PmT5WbP5mQg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "# labels = ['Retained', 'Churned']\n",
        "cm = confusion_matrix(y_train, train_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax,fmt=\".1f\") #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "# ax.xaxis.set_ticklabels(labels)\n",
        "# ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "O0iQo4MAmWF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AUC-ROC:\n",
        "The Receiver Operator Characteristic (ROC) is a probability curve that plots the TPR(True Positive Rate) against the FPR(False Positive Rate) at various threshold values and separates the ‘signal’ from the ‘noise’.\n",
        "\n",
        "the greater the AUC, the better is the performance of the model at different threshold points between positive and negative classes.\n",
        "\n",
        "When AUC is equal to 1, the classifier is able to perfectly distinguish between all Positive and Negative class points. When AUC is equal to 0, the classifier would be predicting all Negatives as Positives and vice versa. When AUC is 0.5, the classifier is not able to distinguish between the Positive and Negative classes.\n",
        "\n",
        "Since the data we are dealing with is unbalanced, accuracy may not be the best evaluation metric to evaluate the model performance.\n",
        "\n",
        "Also, since we are dealing with data related to healthcare, False Negatives are of higher concern than False Positive\n",
        "\n",
        "In other words, it doesn’t matter whether we raise a false alarm but the actual positive cases should not go undetected\n",
        "\n",
        "Considering these points in mind, it is decided that we use Recall as the model evaluation metric."
      ],
      "metadata": {
        "id": "Xf8Qi2knntkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Precision:\n",
        "Number of true positives divided by the number of predicted positives.\n",
        "Precision = True Positive/(True Positive+ False Positive)\n",
        "\n",
        "### Recall (Sensitivity):\n",
        "True positives divided by the total number of actual positives.\n",
        "Recall = True Positive/(True Positive+ False Negative)"
      ],
      "metadata": {
        "id": "5V-OmLNin3Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(metrics.classification_report(train_preds, y_train))\n",
        "# print(\" \")\n",
        "print(metrics.classification_report(y_train, train_preds))\n",
        "print(\" \")\n",
        "\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_train, train_preds))"
      ],
      "metadata": {
        "id": "RhyU2nTAma0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter tunning\n",
        "\n",
        "parameter={\n",
        "      'dual':[False],\n",
        "      'fit_intercept':[True],\n",
        "      'intercept_scaling':[1],\n",
        "      'max_iter':[100]\n",
        "\n",
        "  } \n",
        "\n",
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "gsc_log=GridSearchCV(classifier,param_grid=parameter,cv=5,scoring='neg_mean_absolute_error')\n",
        "\n",
        "# Fit the Algorithm\n",
        "gsc_log.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "l7dFAxZSmmoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "\n",
        "gsc_log_pred=gsc_log.predict(x_test)\n",
        "gsc_log_pred"
      ],
      "metadata": {
        "id": "pVmyqs1bmsAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(gsc_log_pred,y_test)"
      ],
      "metadata": {
        "id": "C-_VU8ERmver"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculates precision for 1:1:100 dataset with 50tp,20fp, 99tp,51fp\n",
        "from sklearn.metrics import precision_score\n",
        "precision = precision_score(y_train,  train_preds,labels=[1,2], average='micro')\n",
        "print('Precision: %.3f' % precision)"
      ],
      "metadata": {
        "id": "PzstPZMAnI3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GridSearchCV is the process of performing hyperparameter tuning in order to determine the optimal values for a given model. As mentioned above, the performance of a model significantly depends on the value of hyperparameters. Note that there is no way to know in advance the best values for hyperparameters so ideally, we need to try all possible values to know the optimal values. Doing this manually could take a considerable amount of time and resources and thus we use GridSearchCV to automate the tuning of hyperparameters."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After applying Logistic to model our precision score is 85%, after hyperparameter tunning using grid searchCV our precision is 50%"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **XGboost Classification**"
      ],
      "metadata": {
        "id": "e-GL1GFtnR1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "# Create an instance of the XGboost classsifier\n",
        "xgb= XGBClassifier(use_label_encoder=None, eval_metric='mlogloss')"
      ],
      "metadata": {
        "id": "B89flG2VNvwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Model fitting\n",
        "xgb.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "BfGqUN7Xnruo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "xgb_pred = xgb.predict(x_test)\n",
        "#  xgb predicted value\n",
        "xgb_pred"
      ],
      "metadata": {
        "id": "oFq_HET2QC8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xgb score check\n",
        "xgb.score(x_train, y_train)"
      ],
      "metadata": {
        "id": "JFswS5BOQM4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "xgb_train=xgb.predict(x_train)\n",
        "print(\"train model\",xgb_train)\n",
        "\n",
        "xgb_pred=xgb.predict(x_test)\n",
        "print(\"Test model \",xgb_pred)"
      ],
      "metadata": {
        "id": "zb3QzOqZQpfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(19,6))\n",
        "plt.plot((xgb_pred), color=\"blue\")\n",
        "plt.plot(np.array((y_test)),color=\"red\")\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G7_77QQXQw-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "cm = confusion_matrix(y_train, xgb_train)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax,fmt=\".1f\") #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(y_train, xgb_train))\n",
        "print(\" \")\n",
        "\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_train, xgb_train))"
      ],
      "metadata": {
        "id": "ooLe3FijRAig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameter={\n",
        "      'booster':['gbtree'],\n",
        "      'verbosity':[2],\n",
        "      'max_depth':[6],\n",
        "      'eta':[0.9]\n",
        "\n",
        "  } \n",
        "\n",
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "xgb_gsv=GridSearchCV(xgb,param_grid=parameter,cv=5,scoring='neg_mean_absolute_error')\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "xgb_gsv.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "OuIs9wDtRPck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_rsc_xgb=xgb_gsv.predict(x_test)\n",
        "y_pred_rsc_xgb"
      ],
      "metadata": {
        "id": "_OlegmySRYtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(y_pred_rsc_xgb,y_test)"
      ],
      "metadata": {
        "id": "g1kx5FysRad3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GridSearchCV is the process of performing hyperparameter tuning in order to determine the optimal values for a given model. As mentioned above, the performance of a model significantly depends on the value of hyperparameters. Note that there is no way to know in advance the best values for hyperparameters so ideally, we need to try all possible values to know the optimal values. Doing this manually could take a considerable amount of time and resources and thus we use GridSearchCV to automate the tuning of hyperparameters."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In Both of the cases there is no improvement"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The confusion matrix is a matrix used to determine the performance of the classification models for a given set of test data. It can only be determined if the true values for test data are known. The matrix itself can be easily understood, but the related terminologies may be confusing. Since it shows the errors in the model performance in the form of a matrix, hence also known as an error matrix."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The confusion matrix is a matrix used to determine the performance of the classification models for a given set of test data. It can only be determined if the true values for test data are known. The matrix itself can be easily understood, but the related terminologies may be confusing. Since it shows the errors in the model performance in the form of a matrix, hence also known as an error matrix. Some features of Confusion matrix are given below:\n",
        "\n",
        "### For the 2 prediction classes of classifiers, the matrix is of 22 table, for 3 classes, it is 33 table, and so on.\n",
        "### The matrix is divided into two dimensions, that are predicted values and actual values along with the total number of predictions.\n",
        "### Predicted values are those values, which are predicted by the model, and actual values are the true values for the given observations."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extreme gradient boosting, or XGBoost, is a potent and well-known gradient boosting technique used to address a wide range of machine learning issues. It is a widely used method for working with huge datasets since it is an implementation of gradient boosting that is especially created to be effective and scalable. XGBoost is a mathematical ensemble learning technique that integrates the results of many weak models to yield a strong prediction. Decision trees, which are trained using gradient boosting, are the weak models in XGBoost. This indicates that the algorithm creates a decision tree at each iteration and fits it to the residuals from the previous iteration."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SHAP**\n",
        "\n",
        "###  SHapley Additive exPlanation (SHAP), may offer model-independent local explainability for tabular, picture, and text datasets.\n",
        "\n",
        "### Shapley values, a notion frequently employed in game theory, are the foundation of SHAP. I will present a straightforward, intuitive explanation of Shapley values and SHAP and place a greater emphasis on the framework's practical applications, despite the fact that the mathematical comprehension of Shapley values can be challenging.\n",
        "\n"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X100 = shap.utils.sample(X, 100) "
      ],
      "metadata": {
        "id": "g0GHLMXeoeuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of column sysBP  using partial dependence plot\n",
        "shap.partial_dependence_plot(\n",
        "    \"sysBP\", classifier.predict, X100, ice=False,\n",
        "    model_expected_value=True, feature_expected_value=True\n",
        ")"
      ],
      "metadata": {
        "id": "uc5jC8P7olC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the SHAP values for the linear model\n",
        "explainer = shap.Explainer(classifier.predict, X100)\n",
        "shap_values = explainer(X)\n",
        "\n",
        "# make a standard partial dependence plot\n",
        "sample_ind = 20\n",
        "shap.partial_dependence_plot(\n",
        "    \"sysBP\", classifier.predict, X100, model_expected_value=True,\n",
        "    feature_expected_value=True, ice=False,\n",
        "    shap_values=shap_values[sample_ind:sample_ind+1,:]\n",
        ")"
      ],
      "metadata": {
        "id": "9N-np96so3eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Water fall visualization\n",
        "shap.plots.waterfall(shap_values[sample_ind], max_display=14)"
      ],
      "metadata": {
        "id": "jp18PV05pY0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Waterfall plots are designed to display explanations for individual predictions, so they expect a single row of an Explanation object as input. The bottom of a waterfall plot starts as the expected value of the model output, and then each row shows how the positive (red) or negative (blue) contribution of each feature moves the value from the expected model output over the background dataset to the model output for this prediction."
      ],
      "metadata": {
        "id": "-68gT9-pprKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot for column totChol\n",
        "shap.plots.scatter(shap_values[:,\"totChol\"])"
      ],
      "metadata": {
        "id": "tQusBL5npyNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit a GAM model to the data\n",
        "\n",
        "model_ebm = interpret.glassbox.ExplainableBoostingRegressor(interactions=0)\n",
        "model_ebm.fit(X, y)\n",
        "\n",
        "# explain the GAM model with SHAP\n",
        "explainer_ebm = shap.Explainer(model_ebm.predict, X100)\n",
        "shap_values_ebm = explainer_ebm(X)"
      ],
      "metadata": {
        "id": "82HT4iQop5pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a standard partial dependence plot\n",
        "sample_ind = 20\n",
        "shap.partial_dependence_plot(\n",
        "    \"sysBP\", classifier.predict, X100, model_expected_value=True,\n",
        "    feature_expected_value=True, ice=False,\n",
        "    shap_values=shap_values[sample_ind:sample_ind+1,:]\n",
        ")"
      ],
      "metadata": {
        "id": "hddwWrlQqVVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the waterfall_plot shows how we get from shap_values.base_values to model.predict(X)[sample_ind]\n",
        "shap.plots.waterfall(shap_values[sample_ind], max_display=14)"
      ],
      "metadata": {
        "id": "k_YI9xCDqdeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Waterfall plots are designed to display explanations for individual predictions, so they expect a single row of an Explanation object as input. The bottom of a waterfall plot starts as the expected value of the model output, and then each row shows how the positive (red) or negative (blue) contribution of each feature moves the value from the expected model output over the background dataset to the model output for this prediction."
      ],
      "metadata": {
        "id": "J1PmP62Oqld4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the waterfall_plot shows how we get from explainer.expected_value to model.predict(X)[sample_ind]\n",
        "shap.plots.beeswarm(shap_values_ebm,order=shap_values.abs.max(0))"
      ],
      "metadata": {
        "id": "9CeqiPioqoVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature importance: Variables are ranked in descending order.\n",
        "### Impact: The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.\n",
        "\n",
        "### Original value: Color shows whether that variable is high (in red) or low (in blue) for that observation.\n",
        "### Correlation: A high level of the “alcohol” content has a high and positive impact on the quality rating. The “high” comes from the red color, and the “positive” impact is shown on the X-axis. Similarly, we will say the “volatile acidity” is negatively correlated with the target variable."
      ],
      "metadata": {
        "id": "XBpFhjKMrc_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.bar(shap_values)"
      ],
      "metadata": {
        "id": "fi_9nq7JrnsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### By default a SHAP bar plot will take the mean absolute value of each feature over all the instances (rows) of the dataset. But the mean absolute value is not the only way to create a global measure of feature importance, we can use any number of transforms. Here we show how using the max absolute value highights the Capital Gain and Capital Loss features, since they have infrewuent but high magnitude effects."
      ],
      "metadata": {
        "id": "NbPk85jYrwoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.bar(shap_values.abs.max(0))"
      ],
      "metadata": {
        "id": "yh7b9hkkr0An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If we are willing to deal with a bit more complexity we can use a beeswarm plot to summarize the entire distribution of SHAP values for each feature."
      ],
      "metadata": {
        "id": "TIJGcPr4r-5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# beeswarm visualization for features\n",
        "shap.plots.beeswarm(shap_values)"
      ],
      "metadata": {
        "id": "2Y8w06o-sDXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### By taking the absolute value and using a solid color we get a compromise between the complexity of the bar plot and the full beeswarm plot. Note that the bar plots above are just summary statistics from the values shown in the beeswarm plots below."
      ],
      "metadata": {
        "id": "aHu7j6xasRdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.beeswarm(shap_values.abs, color=\"shap_red\")"
      ],
      "metadata": {
        "id": "PekSm6TVsU3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scatter plot for realtion beetween age and sysBP\n",
        "shap.plots.scatter(shap_values[:,\"age\"], color=shap_values)"
      ],
      "metadata": {
        "id": "hdbPVGTVsbxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scatter plot for realtion beetween heartrate and sysBP\n",
        "\n",
        "shap.plots.scatter(shap_values[:,\"sysBP\"], color=shap_values)"
      ],
      "metadata": {
        "id": "lQZ6hAHbshya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The dataset contains a total of 3390 entries.\n",
        "### 1923 out of 3390 are female entries.\n",
        "### 1467 out of 3390 are male entries.\n",
        "### 2879 people have 10 years of CHD disease.\n",
        "### 511 People have not 10 years of CHD disease.\n",
        "### The minimum age in the given data set is 32.\n",
        "### The maximum age in the given data set is 70.\n",
        "### A total of 148 people entries are available from the age group 40.\n",
        "### 776 out of 1923 females do smoke.\n",
        "### 1147 out of 1923 females do not smoke.\n",
        "### 911 out of 1467 males do smoking.\n",
        "### 556 out of 1467 males do not smoke.\n",
        "### 32 age group people consume 15 cigarettes per day.\n",
        "### 70 age group people consume 0 cigarettes per day.\n",
        "### 62 age group people have an average of 260 cholesterol measure, which is high among all age groups.\n",
        "### 70 year old people have an average of 169 cholesterol measures, which is low among all age groups.\n",
        "### Heart rate is higher for age group 32 and 69 which is nearly equal to 80.\n",
        "### The lowest heart rate was recorded for age 70 which is 64."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Important Features:\n",
        "### Features which predict CVD will occur:\n",
        "\n",
        "cigs_per_day\n",
        "\n",
        "heartRate\n",
        "\n",
        "age\n",
        "\n",
        "glucose\n",
        "\n",
        "###Features which predict CVD will not occur:\n",
        "\n",
        "education\n",
        "\n",
        "prevalent_stroke"
      ],
      "metadata": {
        "id": "Z40xm3FT2iJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}